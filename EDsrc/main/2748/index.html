<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gemini Live Translator (Standalone)</title>
    <!-- chat Hier ist die **Standalone-Lösung ("Single File App")**.

Ich habe die Logik aus App 2 (TypeScript/React) extrahiert und in **natives, reines JavaScript** portiert. Es benötigt keine Installation, kein `npm`, kein `node_modules`.

**Anleitung:**

1.  Erstelle einen Ordner.
2.  Speichere den Code unten als `index.html`.
3.  **WICHTIG:** Da Browser den Zugriff auf das Mikrofon bei reinen `file://`-Dateien oft blockieren, musst du es über einen lokalen Server starten.
      * Wenn du Python hast: `python -m http.server` im Ordner ausführen.
      * Dann im Browser: `http://localhost:8000` öffnen.
4.  Gib deinen **Google API Key** ein (benötigt Zugriff auf `gemini-2.0-flash-exp` oder ähnliche Modelle).

### Der Code (`index.html`)

```html
-->
    <style>
        :root { --bg: #0e1117; --panel: #1e212b; --accent: #4a9eff; --text: #e0e0e0; --err: #ff4a4a; }
        body { background: var(--bg); color: var(--text); font-family: 'Segoe UI', monospace; margin: 0; padding: 20px; display: flex; flex-direction: column; height: 100vh; box-sizing: border-box; }
        .container { max-width: 800px; margin: 0 auto; width: 100%; display: flex; flex-direction: column; gap: 15px; }
        
        /* Header & Config */
        .card { background: var(--panel); padding: 15px; border-radius: 8px; border: 1px solid #333; }
        h1 { margin: 0 0 10px 0; font-size: 1.2rem; color: var(--accent); }
        
        .row { display: flex; gap: 10px; margin-bottom: 10px; align-items: center; }
        input, select, button { background: #000; color: #fff; border: 1px solid #444; padding: 8px; border-radius: 4px; font-family: monospace; }
        input { flex: 1; }
        button { cursor: pointer; font-weight: bold; transition: background 0.2s; }
        button:hover { background: #333; }
        button.active { background: var(--err); color: #fff; border-color: var(--err); }
        button.start { background: #28a745; border-color: #28a745; }

        /* Visualizer */
        canvas { width: 100%; height: 60px; background: #000; border-radius: 4px; border: 1px solid #333; }

        /* Logs */
        #log { flex: 1; background: #000; border: 1px solid #333; border-radius: 4px; padding: 10px; overflow-y: auto; font-family: monospace; font-size: 0.85rem; white-space: pre-wrap; height: 300px; }
        .log-user { color: #4a9eff; }
        .log-model { color: #a8ff4a; }
        .log-sys { color: #888; font-style: italic; }
        .log-err { color: #ff4a4a; }

        /* Settings grid */
        .grid { display: grid; grid-template-columns: 1fr 1fr; gap: 10px; }
    </style>
</head>
<body>

<div class="container">
    <div class="card">
        <h1>Gemini Live Translation Engine (Native Audio)</h1>
        
        <div class="row">
            <input type="password" id="apiKey" placeholder="Enter Google API Key (AI Studio)" value="">
        </div>

        <div class="grid">
            <select id="audioInput"><option>Loading devices...</option></select>
            <select id="voice">
                <option value="Puck">Voice: Puck</option>
                <option value="Charon">Voice: Charon</option>
                <option value="Kore">Voice: Kore</option>
                <option value="Fenrir">Voice: Fenrir</option>
                <option value="Aoede">Voice: Aoede</option>
            </select>
        </div>
        
        <div class="row" style="margin-top: 10px;">
            <button id="btnToggle" class="start">START LIVE SESSION</button>
            <div style="flex:1; text-align:right; font-size:0.8rem; color:#666;" id="status">Ready</div>
        </div>
        
        <canvas id="vis"></canvas>
    </div>

    <div id="log"></div>
</div>

<script>
/**
 * CORE LOGIC - Extracted from App 2
 * Handles: AudioContext (16kHz), WebSocket (Gemini), PCM Conversion
 */

// --- CONFIG ---
const MODEL = "models/gemini-2.0-flash-exp"; // Or "gemini-2.0-flash-exp"
const HOST = "generativelanguage.googleapis.com";
const SAMPLE_RATE = 24000; // Gemini Native often prefers 24k output, input usually 16k

// --- STATE ---
let ctx = null;           // AudioContext
let ws = null;            // WebSocket
let micStream = null;     // MediaStream
let inputProcessor = null;// ScriptProcessor (Recorder)
let isConnected = false;
let audioQueue = [];      // Queue for PCM playback
let nextStartTime = 0;    // For drift correction
let visualizerInterval = null;

// --- DOM ELEMENTS ---
const ui = {
    key: document.getElementById('apiKey'),
    mic: document.getElementById('audioInput'),
    voice: document.getElementById('voice'),
    btn: document.getElementById('btnToggle'),
    log: document.getElementById('log'),
    cvs: document.getElementById('vis'),
    status: document.getElementById('status')
};
const cvsCtx = ui.cvs.getContext('2d');

// --- LOGGER ---
function log(msg, type='sys') {
    const d = document.createElement('div');
    d.className = `log-${type}`;
    d.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
    ui.log.appendChild(d);
    ui.log.scrollTop = ui.log.scrollHeight;
}

// --- AUDIO UTILS ---
// Convert Float32 (WebAudio) to Int16 (PCM)
function floatTo16BitPCM(input) {
    const output = new Int16Array(input.length);
    for (let i = 0; i < input.length; i++) {
        const s = Math.max(-1, Math.min(1, input[i]));
        output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return output;
}

// Base64 Helpers
function arrayBufferToBase64(buffer) {
    let binary = '';
    const bytes = new Uint8Array(buffer);
    for (let i = 0; i < bytes.length; i++) binary += String.fromCharCode(bytes[i]);
    return btoa(binary);
}

function base64ToArrayBuffer(base64) {
    const binary_string = window.atob(base64);
    const len = binary_string.length;
    const bytes = new Uint8Array(len);
    for (let i = 0; i < len; i++) bytes[i] = binary_string.charCodeAt(i);
    return bytes.buffer;
}

// --- DEVICE MANAGEMENT ---
async function loadDevices() {
    ui.mic.innerHTML = "";
    try {
        const devs = await navigator.mediaDevices.enumerateDevices();
        const inputs = devs.filter(d => d.kind === 'audioinput');
        inputs.forEach(d => {
            const opt = document.createElement('option');
            opt.value = d.deviceId;
            opt.textContent = d.label || `Mic ${d.deviceId.slice(0,5)}`;
            ui.mic.appendChild(opt);
        });
        if(inputs.length === 0) ui.mic.innerHTML = "<option>No Mics Found</option>";
    } catch(e) {
        log("Permission needed to list devices", "err");
    }
}
navigator.mediaDevices.getUserMedia({audio:true}).then(s=>{s.getTracks().forEach(t=>t.stop()); loadDevices();}).catch(e=>log(e.message,"err"));

// --- AUDIO ENGINE (RECORDING) ---
async function startAudio() {
    ctx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 }); // Try to force 16k
    
    // Resume context if suspended (Chrome policy)
    if (ctx.state === 'suspended') await ctx.resume();

    const constraints = {
        audio: {
            deviceId: { exact: ui.mic.value },
            channelCount: 1,
            sampleRate: 16000, // Request 16k
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
        }
    };

    micStream = await navigator.mediaDevices.getUserMedia(constraints);
    const source = ctx.createMediaStreamSource(micStream);
    
    // Processor: Buffer size 2048 or 4096. 
    // WARNING: ScriptProcessor is deprecated but works everywhere without separate files. 
    // Ideally use AudioWorklet for production.
    inputProcessor = ctx.createScriptProcessor(4096, 1, 1);
    
    inputProcessor.onaudioprocess = (e) => {
        if (!isConnected) return;

        const inputData = e.inputBuffer.getChannelData(0);
        
        // 1. Resample logic if context isn't 16k (Skip for simplicity, rely on ctx being close)
        // 2. Convert to PCM Int16
        const pcm16 = floatTo16BitPCM(inputData);
        
        // 3. Send to WS
        sendAudioChunk(pcm16);
        
        // 4. Visualize
        drawVis(inputData);
    };

    source.connect(inputProcessor);
    inputProcessor.connect(ctx.destination); // Mute locally? No, destination needed for processing to fire in some browsers
    // Actually, to mute loopback, we connect processor to destination but don't output audio in process.
    // The processor output is usually silence unless we copy input to output.
}

function stopAudio() {
    if (micStream) micStream.getTracks().forEach(t => t.stop());
    if (inputProcessor) { inputProcessor.disconnect(); inputProcessor.onaudioprocess = null; }
    if (ctx) ctx.close();
}

// --- GEMINI CLIENT ---
function connect() {
    const key = ui.key.value.trim();
    if (!key) return alert("API Key Required!");

    ui.btn.textContent = "Connecting...";
    ui.status.textContent = "Handshake...";
    
    // Construct URL
    const url = `wss://${HOST}/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent?key=${key}`;
    
    ws = new WebSocket(url);

    ws.onopen = async () => {
        log("Connected to Gemini Live", "sys");
        ui.btn.textContent = "STOP SESSION";
        ui.btn.className = "active";
        ui.status.textContent = "Live";
        isConnected = true;
        nextStartTime = 0; // Reset playback clock

        // 1. Setup Message (JSON)
        const setupMsg = {
            setup: {
                model: MODEL,
                generation_config: {
                    response_modalities: ["AUDIO"], // We want Audio back
                    speech_config: {
                        voice_config: { prebuilt_voice_config: { voice_name: ui.voice.value } }
                    }
                },
                system_instruction: {
                  parts: [{ text: "You are a live translator. You translate immediately. Do not chatter. Just translate." }]
                }
            }
        };
        ws.send(JSON.stringify(setupMsg));
        
        // 2. Start Mic
        await startAudio();
    };

    ws.onmessage = async (evt) => {
        let msg;
        if (evt.data instanceof Blob) {
            msg = JSON.parse(await evt.data.text());
        } else {
            msg = JSON.parse(evt.data);
        }

        if (msg.serverContent) {
            // A. Audio Turn
            if (msg.serverContent.modelTurn) {
                const parts = msg.serverContent.modelTurn.parts;
                for (const p of parts) {
                    if (p.inlineData && p.inlineData.mimeType.startsWith('audio')) {
                        // Audio Chunk received
                        queueAudio(p.inlineData.data);
                    }
                }
            }
            // B. Turn Complete (Interrupt logic could go here)
            if (msg.serverContent.turnComplete) {
                // log("Turn complete", "sys");
            }
        }
    };

    ws.onclose = () => {
        log("Disconnected", "sys");
        disconnect();
    };

    ws.onerror = (e) => {
        log("WebSocket Error", "err");
        console.error(e);
        disconnect();
    };
}

function sendAudioChunk(pcmData) {
    if (ws && ws.readyState === WebSocket.OPEN) {
        // Prepare BidiRealtimeInput
        const b64 = arrayBufferToBase64(pcmData.buffer);
        const msg = {
            realtime_input: {
                media_chunks: [{
                    mime_type: "audio/pcm",
                    data: b64
                }]
            }
        };
        ws.send(JSON.stringify(msg));
    }
}

function disconnect() {
    isConnected = false;
    if (ws) { ws.close(); ws = null; }
    stopAudio();
    ui.btn.textContent = "START LIVE SESSION";
    ui.btn.className = "start";
    ui.status.textContent = "Idle";
}

// --- PLAYBACK ENGINE ---
function queueAudio(base64Data) {
    // 1. Decode
    const arrayBuffer = base64ToArrayBuffer(base64Data);
    const pcm16 = new Int16Array(arrayBuffer);
    
    // 2. Create Float32 Buffer for AudioContext
    const float32 = new Float32Array(pcm16.length);
    for (let i=0; i<pcm16.length; i++) {
        float32[i] = pcm16[i] / 32768; // Normalize to -1..1
    }

    // 3. Schedule Playback
    // Ensure we are using the correct context (Sample Rate of response is usually 24k)
    // We can resample or let AudioBuffer handle it. 
    // Gemini Output is PCM 24000Hz usually.
    
    const audioBuffer = ctx.createBuffer(1, float32.length, 24000); 
    audioBuffer.copyToChannel(float32, 0);

    const source = ctx.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(ctx.destination);

    // Drift Correction logic
    const currentTime = ctx.currentTime;
    if (nextStartTime < currentTime) nextStartTime = currentTime;
    
    source.start(nextStartTime);
    nextStartTime += audioBuffer.duration;
}

// --- VISUALIZER ---
function drawVis(dataArray) {
    const width = ui.cvs.width;
    const height = ui.cvs.height;
    cvsCtx.fillStyle = '#000';
    cvsCtx.fillRect(0, 0, width, height);
    
    cvsCtx.lineWidth = 2;
    cvsCtx.strokeStyle = '#4a9eff';
    cvsCtx.beginPath();
    
    const sliceWidth = width * 1.0 / dataArray.length;
    let x = 0;
    
    for(let i = 0; i < dataArray.length; i++) {
        const v = dataArray[i]; // -1 to 1
        const y = (v + 1) / 2 * height;
        if(i === 0) cvsCtx.moveTo(x, y);
        else cvsCtx.lineTo(x, y);
        x += sliceWidth;
    }
    cvsCtx.stroke();
}

// --- EVENTS ---
ui.btn.onclick = () => {
    if (isConnected) disconnect();
    else connect();
};

</script>
</body>
</html>