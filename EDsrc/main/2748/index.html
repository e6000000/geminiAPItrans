<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <title>Gemini Live (Safe Mode)</title>
    <script src="config.js"></script>
    <style>
        :root { --bg: #0b1120; --panel: #1e293b; --accent: #3b82f6; --text: #f1f5f9; --green: #10b981; --red: #ef4444; }
        body { font-family: monospace; background: var(--bg); color: var(--text); display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; }
        .panel { background: var(--panel); padding: 30px; border-radius: 12px; width: 90%; max-width: 600px; border: 1px solid #334155; }
        h1 { text-align: center; color: var(--accent); margin-top: 0; }
        .status-box { background: #000; padding: 10px; border-radius: 6px; font-size: 0.8rem; height: 100px; overflow-y: auto; border: 1px solid #333; margin-bottom: 20px; white-space: pre-wrap; color: #0f0; }
        button { width: 100%; padding: 15px; font-weight: bold; font-size: 1.1rem; cursor: pointer; border: none; border-radius: 6px; margin-top: 5px; }
        #btnStart { background: var(--green); color: #000; }
        #btnStop { background: var(--red); color: white; display: none; }
        select { width: 100%; padding: 10px; margin-bottom: 10px; background: #0f172a; color: white; border: 1px solid #333; border-radius: 6px; }
        label { font-size: 0.7rem; color: #888; text-transform: uppercase; }
    </style>
</head>
<body>

    <div class="panel">
        <h1>Gemini Safe Mode üõ°Ô∏è</h1>
        <div style="text-align:center; color:#888; margin-bottom:15px; font-size:0.8rem;">
            Sammelt 1-2 Sekunden Audio -> 1 Paket.<br>Verhindert "Too Many Requests".
        </div>

        <div id="console" class="status-box">Bereit.</div>

        <label>Mikrofon</label>
        <select id="micSelect"><option>Lade...</option></select>
        
        <label>Ausgabe</label>
        <select id="speakerSelect"><option>Standard</option></select>

        <button id="btnStart">Safe Start</button>
        <button id="btnStop">Stop</button>
    </div>

<script>
    // --- CONFIG ---
    // Wir nutzen explizit das "Flash" Modell, da es am wenigsten Quota verbraucht
    const URL_API = "wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent";
    const MODEL = "models/gemini-2.0-flash-exp";

    // --- VARIABLES ---
    let ws = null;
    let audioCtx = null;
    let processor = null;
    let micStream = null;
    let inputSource = null;
    let audioBufferQueue = []; // Hier sammeln wir PCM Daten
    let isConnected = false;
    let nextPlayTime = 0;

    // UI
    const ui = {
        log: document.getElementById('console'),
        btnStart: document.getElementById('btnStart'),
        btnStop: document.getElementById('btnStop'),
        mic: document.getElementById('micSelect'),
        spk: document.getElementById('speakerSelect')
    };

    function log(msg) {
        ui.log.textContent = `> ${msg}\n` + ui.log.textContent;
    }

    // 1. INIT DEVICES
    async function init() {
        try {
            await navigator.mediaDevices.getUserMedia({ audio: true });
            const devs = await navigator.mediaDevices.enumerateDevices();
            ui.mic.innerHTML = ''; ui.spk.innerHTML = '';
            devs.forEach(d => {
                const o = document.createElement('option');
                o.value = d.deviceId;
                o.text = d.label || d.kind;
                if(d.kind==='audioinput') ui.mic.appendChild(o);
                if(d.kind==='audiooutput') ui.spk.appendChild(o);
            });
            // Auto Select Voicemeeter
            const vm = Array.from(ui.mic.options).find(o=>o.text.includes("VoiceMeeter Out"));
            if(vm) ui.mic.value = vm.value;
        } catch(e) { log("Kein Mic Zugriff!"); }
    }
    init();

    // 2. START
    ui.btnStart.onclick = async () => {
        if(typeof CONFIG_API_KEY === 'undefined') return alert("config.js fehlt");
        
        // Audio Context (16kHz Pflicht f√ºr Gemini Input)
        audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
        if(audioCtx.setSinkId && ui.spk.value) try { await audioCtx.setSinkId(ui.spk.value); } catch(e){}

        log("Starte Audio...");
        
        try {
            micStream = await navigator.mediaDevices.getUserMedia({
                audio: { deviceId: { exact: ui.mic.value }, channelCount: 1, sampleRate: 16000 }
            });
            
            // WS Connect
            log("Verbinde WS...");
            ws = new WebSocket(`${URL_API}?key=${CONFIG_API_KEY}`);
            
            ws.onopen = () => {
                log("Verbunden! Sende Setup...");
                isConnected = true;
                ui.btnStart.style.display = 'none';
                ui.btnStop.style.display = 'inline-block';
                
                // Handshake
                const setup = {
                    setup: {
                        model: MODEL,
                        generation_config: { response_modalities: ["AUDIO"], speech_config: { voice_config: { prebuilt_voice_config: { voice_name: "Puck" } } } },
                        system_instruction: { parts: [{ text: "Translate to German." }] }
                    }
                };
                ws.send(JSON.stringify(setup));
                
                // Audio Processing starten
                startRecording();
            };

            ws.onmessage = async (e) => {
                const d = JSON.parse(e.data instanceof Blob ? await e.data.text() : e.data);
                if(d.serverContent?.modelTurn?.parts?.[0]?.inlineData) {
                    playAudio(d.serverContent.modelTurn.parts[0].inlineData.data);
                }
            };

            ws.onclose = (e) => stop("Closed: " + e.code);
            ws.onerror = (e) => stop("Error");

        } catch(e) { stop("Init Fail: " + e); }
    };

    // 3. SAFE RECORDING (Der Trick)
    function startRecording() {
        inputSource = audioCtx.createMediaStreamSource(micStream);
        processor = audioCtx.createScriptProcessor(4096, 1, 1);
        
        inputSource.connect(processor);
        processor.connect(audioCtx.destination);

        let sampleCounter = 0;
        // Wir senden erst, wenn wir 24.000 Samples haben (~1.5 Sekunden)
        // Das garantiert, dass wir das Rate Limit nicht sprengen.
        const SEND_THRESHOLD = 24000; 

        processor.onaudioprocess = (e) => {
            if(!isConnected || !ws) return;

            const input = e.inputBuffer.getChannelData(0);
            
            // Daten sammeln (Float32 -> Int16)
            for (let i = 0; i < input.length; i++) {
                const s = Math.max(-1, Math.min(1, input[i]));
                // Einfacher Push in Array ist langsam, aber sicher f√ºr Debugging
                audioBufferQueue.push(s < 0 ? s * 0x8000 : s * 0x7FFF);
            }

            sampleCounter += input.length;

            // Erst senden, wenn voll!
            if (sampleCounter >= SEND_THRESHOLD) {
                log(`Sende Paket (${audioBufferQueue.length} samples)...`);
                
                // Array -> TypedArray -> Base64
                const pcm16 = new Int16Array(audioBufferQueue);
                const u8 = new Uint8Array(pcm16.buffer);
                
                // Base64 manuell (btoa mag keine gro√üen Arrays, daher Chunking)
                let binary = '';
                // Optimierung: Nur erste und letzte Bytes loggen, sonst st√ºrzt Browser ab bei gro√üem String
                // Wir nutzen hier einen Trick f√ºr gro√üe Base64 Strings
                const CHUNK_SZ = 0x8000;
                for(let i=0; i<u8.length; i+=CHUNK_SZ) {
                    binary += String.fromCharCode.apply(null, u8.subarray(i, i+CHUNK_SZ));
                }
                const b64 = btoa(binary);

                ws.send(JSON.stringify({
                    realtime_input: { media_chunks: [{ mime_type: "audio/pcm", data: b64 }] }
                }));

                // Reset
                audioBufferQueue = [];
                sampleCounter = 0;
            }
        };
    }

    // 4. PLAYBACK
    function playAudio(b64) {
        log("<< Audio empfangen");
        const bin = atob(b64);
        const bytes = new Uint8Array(bin.length);
        for(let i=0; i<bin.length; i++) bytes[i] = bin.charCodeAt(i);
        const pcm = new Int16Array(bytes.buffer);
        const float = new Float32Array(pcm.length);
        for(let i=0; i<pcm.length; i++) float[i] = pcm[i] / 32768;

        const buf = audioCtx.createBuffer(1, float.length, 24000);
        buf.copyToChannel(float, 0);
        
        const src = audioCtx.createBufferSource();
        src.buffer = buf;
        src.connect(audioCtx.destination);
        
        const now = audioCtx.currentTime;
        if(nextStartTime < now) nextStartTime = now;
        src.start(nextStartTime);
        nextStartTime += buf.duration;
    }

    // 5. STOP
    ui.btnStop.onclick = () => { if(ws) ws.close(); stop("User Stop"); };

    function stop(msg) {
        log("STOP: " + msg);
        isConnected = false;
        if(processor) { processor.disconnect(); processor.onaudioprocess = null; }
        if(micStream) micStream.getTracks().forEach(t=>t.stop());
        
        ui.btnStart.style.display = 'block';
        ui.btnStop.style.display = 'none';
        
        audioBufferQueue = [];
    }

</script>
</body>
</html>